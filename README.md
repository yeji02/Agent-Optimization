# 🧠 Agent 기반 시뮬레이터 내부 함수 최적화 예제

## 📌 프로젝트 개요
기존의 시뮬레이션 환경에서는 에이전트와 환경이 고정된 규칙에 따라 움직이며,  
**Atomic 모델 내부 함수는 정적인 로직**만 수행하는 한계가 있었습니다.  
본 프로젝트는 xDEVS 기반 시뮬레이터에서 **에이전트의 내부 함수 자체를 강화학습(Q-learning)으로 최적화**하는 새로운 방식을 제안합니다.

즉, 외부에서 에이전트 행동을 제어하는 것이 아니라,  
시뮬레이터 내부의 `propose_action`, `learn_from`, `deltext`, `lambdaf` 함수가 **Q-learning 알고리즘에 의해 스스로 학습하고 개선**되도록 구성했습니다.  
이를 통해 에이전트가 동적 환경 속에서 **스스로 최적 경로를 학습**하고, 충돌을 피하며 목표 지점에 도달할 수 있습니다.

---

## 🏗 시뮬레이션 구성

| 구성 요소 | 설명 |
|-----------|------|
| **building.py** | 문자 격자 맵을 정의 (`.` = 빈칸, `#` = 장애물). 좌표 유효성 검사 및 한 스텝 전이 규칙 설정. 잘못된 이동(벽/맵 밖)은 -5.0 패널티 후 제자리 복귀, 정상 이동은 -1.9, 목표 도달 시 +100 보상 부여. |
| **simulator.py** | 여러 에이전트의 행동을 동기화하여 시뮬레이션을 진행. 각 에이전트의 이동 의도를 수집하고, 동시 충돌(정점·엣지 스왑) 탐지 및 처리 후 보상을 적용. `run_episode` 루프에서 학습 호출. |
| **model.py** | xDEVS의 Atomic 클래스를 상속한 **Agent 모델** 구현. 내부에 Q-learning 알고리즘을 직접 내장하여, 행동 선택(`propose_action`)과 학습(`learn_from`)을 수행. 에이전트가 독립적으로 정책을 개선함. |
| **main.py** | 환경 설정, 하이퍼파라미터 설정, 에이전트 및 맵 생성, 학습 루프, 로깅 및 시각화를 담당하는 실행 스크립트. |

---

## 🧠 Agent 학습 방식

- **행동 공간**: 이동 8방향(대각선 포함) + 제자리 → 총 9가지  
- **보상 체계**  
  - 벽 또는 맵 밖 이동: -5.0  
  - 단순 이동: -1.0  
  - 목표 도달: +100.0  

- **에피소드 루프 (run_episode)**  
  1. 각 에이전트의 이동 의도 수집 (want, reward0, reached)  
  2. 정점(Vertex) 및 엣지(Edge swap) 충돌 동시 탐지 및 처리  
  3. building의 step_outcome 결과 적용 및 Q-learning 학습 호출  
  4. 목표 도착 여부(done flag) 확인 및 로그 기록

- **학습 알고리즘**: Tabular Q-learning  
  - ε-greedy 정책 사용  
  - 상태는 에이전트 위치 및 맵 상태 기반  
  - 학습은 각 에이전트의 내부 함수에서 독립적으로 수행

---

## 🏆 학습 결과 요약

- 15×20 크기의 격자 맵에서 10개의 에이전트가 1,500회 학습 진행  
- 학습 진행 중 step 수 평균, reward 평균, success rate, invalid episode 수, epsilon 등을 주기적으로 로깅  
- 학습 후 에이전트들은
  - 벽이나 장애물에 충돌하지 않고
  - 같은 칸을 동시에 점유하지 않으며
  - **최단 경로로 목표 지점에 도달**하는 정책을 스스로 학습
- reward와 success rate 모두 점진적으로 증가하며 안정적으로 수렴함

---

## 📂 주요 파일 설명

| 파일명 | 설명 |
|--------|------|
| **building.py** | 맵 정보 및 보상 규칙 정의. 벽, 장애물, 목표 지점에 대한 전이 규칙 설정. |
| **simulator.py** | 여러 에이전트의 동시 행동 시뮬레이션 및 충돌 처리, 학습 루프 관리. |
| **model.py** | xDEVS Atomic을 상속한 Agent 모델. Q-learning 내장 및 행동/학습 로직 정의. |
| **visualizer.py** | 학습 결과와 에이전트의 경로를 **시각적으로 출력**하는 모듈. 맵 형태, 장애물, 에이전트 위치 및 이동 경로를 matplotlib 등을 통해 시각화하여 학습 경향과 충돌 여부 등을 직관적으로 확인할 수 있음. |
| **main.py** | 전체 학습 파이프라인 실행 스크립트. 환경 생성, 학습 루프, 로그 및 시각화 포함. |

---

## 📌 요약

| 항목 | 내용 |
|------|------|
| 프레임워크 | xDEVS (Python) |
| 학습 대상 | Atomic Agent의 내부 함수 (`propose_action`, `learn_from`, 등) |
| 알고리즘 | Tabular Q-learning |
| 환경 | 문자 격자 기반 맵 (빈칸·장애물) |
| 학습 결과 | 충돌 없는 최단 경로 학습, 성공률 및 보상 향상 |

---

## 📚 참고
- xDEVS: [https://github.com/iscar-ucm/xdevs](https://github.com/iscar-ucm/xdevs)  
- Q-learning: Watkins & Dayan, *Q-learning*, Machine Learning, 1992.  
- 문의: yeji11138@seoultech.ac.kr
